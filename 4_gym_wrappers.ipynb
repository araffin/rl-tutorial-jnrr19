{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stable_baselines_gym_wrappers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"(https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/4_gym_wrappers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8lIXBiHRYb6",
        "colab_type": "text"
      },
      "source": [
        "# Gym wrapper and hyperparameter tuning with Stable Baselines\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n",
        "\n",
        "\n",
        "\n",
        "You will also see that finding good hyperparameters is key to success in RL.\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owKXXp8rRZI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install swig\n",
        "!pip install stable-baselines[mpi]==2.8.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18ivrnsaSWUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from stable_baselines import A2C, SAC, PPO2, TD3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PytOtL9GdmrE",
        "colab_type": "text"
      },
      "source": [
        "# The importance of hyperparameter tuning\n",
        "\n",
        "When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc. \n",
        "Poor choice of hyper-parameters can lead to poor/unstable convergence. This challenge is compounded by the variability in performance across random seeds (used to initialize the network weights and the environment).\n",
        "\n",
        "Here we demonstrate on a toy example the [Soft Actor Critic](https://arxiv.org/abs/1801.01290) algorithm applied in the Pendulum environment. Note the change in performance between the default and \"tuned\" parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5oVvYHwdnYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, env, num_episodes=100):\n",
        "    # This function will only work for a single Environment\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            action, _states = model.predict(obs)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    return mean_episode_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a0v3fgwe54j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_env = gym.make('Pendulum-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WRR7kmIeqEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1).learn(8000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQbDcbEheqWj",
        "colab_type": "code",
        "outputId": "4558fe4f-5ed3-4a1e-d13c-91247936b817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "evaluate(default_model, eval_env, num_episodes=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1199.1561304849065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smMdkZnvfL1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tuned_model = SAC('MlpPolicy', 'Pendulum-v0', batch_size=256, verbose=1, policy_kwargs=dict(layers=[256, 256])).learn(8000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN05_Io8fMAr",
        "colab_type": "code",
        "outputId": "163b8928-fb4b-483e-ea16-97f8becfc049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "evaluate(tuned_model, eval_env, num_episodes=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-161.53179911719837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKwupU-Jgxjm",
        "colab_type": "text"
      },
      "source": [
        "# Gym and VecEnv wrappers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4AAfmISQIA",
        "colab_type": "text"
      },
      "source": [
        "## Anatomy of a gym wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTS9e9hTzZZ",
        "colab_type": "text"
      },
      "source": [
        "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
        "\n",
        "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
        "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYo0C0TQSL3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(CustomWrapper, self).__init__(env)\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zeGuyICUN26",
        "colab_type": "text"
      },
      "source": [
        "## First example: limit the episode length\n",
        "\n",
        "One practical use case of a wrapper is when you want to limit the number of steps by episode, for that you will need to overwrite the `done` signal when the limit is reached. It is also a good practice to pass that information in the `info` dictionnary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb2U4_K6SNUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TimeLimitWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  :param max_steps: (int) Max number of steps per episode\n",
        "  \"\"\"\n",
        "  def __init__(self, env, max_steps=100):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(TimeLimitWrapper, self).__init__(env)\n",
        "    self.max_steps = max_steps\n",
        "    # Counter of steps per episode\n",
        "    self.current_step = 0\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    # Reset the counter\n",
        "    self.current_step = 0\n",
        "    return self.env.reset()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    self.current_step += 1\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    # Overwrite the done signal when \n",
        "    if self.current_step >= self.max_steps:\n",
        "      done = True\n",
        "      # Update the info dict to signal that the limit was exceeded\n",
        "      info['time_limit_reached'] = True\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZufaUJwVM9w",
        "colab_type": "text"
      },
      "source": [
        "#### Test the wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szZ43D5PVB07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "\n",
        "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
        "env = PendulumEnv()\n",
        "# Wrap the environment\n",
        "env = TimeLimitWrapper(env, max_steps=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cencka9iVg9V",
        "colab_type": "code",
        "outputId": "366d9def-6e29-4974-fdcb-914dca5ff8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "n_steps = 0\n",
        "while not done:\n",
        "  # Take random actions\n",
        "  random_action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(random_action)\n",
        "  n_steps += 1\n",
        "\n",
        "print(n_steps, info)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 {'time_limit_reached': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMYA63sV9aA",
        "colab_type": "text"
      },
      "source": [
        "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIIJbSyQW9R-",
        "colab_type": "text"
      },
      "source": [
        "## Second example: normalize actions\n",
        "\n",
        "It is usually a good idea to normalize observations and actions before giving it to the agent, this prevent [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n",
        "\n",
        "In this example, we are going to normalize the action space of *Pendulum-v0* so it lies in [-1, 1] instead of [-2, 2].\n",
        "\n",
        "Note: here we are dealing with continuous actions, hence the `gym.Box` space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5E6kZfzW8vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NormalizeActionWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    # Retrieve the action space\n",
        "    action_space = env.action_space\n",
        "    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
        "    # Retrieve the max/min values\n",
        "    self.low, self.high = action_space.low, action_space.high\n",
        "\n",
        "    # We modify the action space, so all actions will lie in [-1, 1]\n",
        "    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n",
        "\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(NormalizeActionWrapper, self).__init__(env)\n",
        "  \n",
        "  def rescale_action(self, scaled_action):\n",
        "      \"\"\"\n",
        "      Rescale the action from [-1, 1] to [low, high]\n",
        "      (no need for symmetric action space)\n",
        "      :param scaled_action: (np.ndarray)\n",
        "      :return: (np.ndarray)\n",
        "      \"\"\"\n",
        "      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    # Reset the counter\n",
        "    return self.env.reset()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    # Rescale action from [-1, 1] to original [low, high] interval\n",
        "    rescaled_action = self.rescale_action(action)\n",
        "    obs, reward, done, info = self.env.step(rescaled_action)\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJ0eahNaR6K",
        "colab_type": "text"
      },
      "source": [
        "#### Test before rescaling actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEnjBwisaQIx",
        "colab_type": "code",
        "outputId": "46b476fd-feb8-43a6-8dbf-3865142adf0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "original_env = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "print(original_env.action_space.low)\n",
        "for _ in range(10):\n",
        "  print(original_env.action_space.sample())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.]\n",
            "[-0.79838526]\n",
            "[0.1980023]\n",
            "[1.7232748]\n",
            "[0.08304575]\n",
            "[-0.9311719]\n",
            "[1.5095952]\n",
            "[-0.512325]\n",
            "[-1.9944665]\n",
            "[-1.0092599]\n",
            "[-0.727066]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvcll2L3afVd",
        "colab_type": "text"
      },
      "source": [
        "#### Test the NormalizeAction wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsCM9AUGaeBN",
        "colab_type": "code",
        "outputId": "9d43ed0f-402b-4114-a453-78bfd731e98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "env = NormalizeActionWrapper(gym.make(\"Pendulum-v0\"))\n",
        "\n",
        "print(env.action_space.low)\n",
        "\n",
        "for _ in range(10):\n",
        "  print(env.action_space.sample())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.]\n",
            "[-0.90638727]\n",
            "[0.9414629]\n",
            "[-0.9922793]\n",
            "[-0.6428401]\n",
            "[0.2257335]\n",
            "[-0.8372608]\n",
            "[0.763793]\n",
            "[0.4392403]\n",
            "[0.93277997]\n",
            "[0.01527109]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5h5kk2mbGNs",
        "colab_type": "text"
      },
      "source": [
        "#### Test with a RL algorithm\n",
        "\n",
        "We are going to use the Monitor wrapper of stable baselines, wich allow to monitor training stats (mean episode reward, mean episode length)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9FNCN8ybOVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.common.vec_env import DummyVecEnv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wutM3c1GbfGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Monitor(gym.make('Pendulum-v0'), filename=None, allow_early_resets=True)\n",
        "env = DummyVecEnv([lambda: env])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cxnE5bdaQ_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = A2C(\"MlpPolicy\", env, verbose=1).learn(int(1000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJFSM-Drb3Wc",
        "colab_type": "text"
      },
      "source": [
        "With the action wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GszFZthob2wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_env = Monitor(gym.make('Pendulum-v0'), filename=None, allow_early_resets=True)\n",
        "# Note that we can use multiple wrappers\n",
        "normalized_env = NormalizeActionWrapper(normalized_env)\n",
        "normalized_env = DummyVecEnv([lambda: normalized_env])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrKJEO4NcIMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BxqXd_6dpJx",
        "colab_type": "text"
      },
      "source": [
        "### Additional wrappers: VecEnvWrappers\n",
        "\n",
        "In the same vein as gym wrappers, stable baselines provide wrappers for `VecEnv`. Among the different that exist (and you can create your own), you should know: \n",
        "\n",
        "- VecNormalize: it computes a running mean and standard deviation to normalize observation and returns\n",
        "- VecFrameStack: it stacks several consecutive observations (useful to integrate time in the observation, e.g. sucessive frame of an atari game)\n",
        "\n",
        "More info in the [documentation](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#wrappers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuIcbfv3g9dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines.common.vec_env import VecNormalize, VecFrameStack\n",
        "\n",
        "env = DummyVecEnv([lambda: gym.make(\"Pendulum-v0\")])\n",
        "normalized_vec_env = VecNormalize(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PAbu21pg90A",
        "colab_type": "code",
        "outputId": "6af7e8ed-0708-44b1-e29f-3456a32cb72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "obs = normalized_vec_env.reset()\n",
        "for _ in range(10):\n",
        "  action = [normalized_vec_env.action_space.sample()]\n",
        "  obs, reward, _, _ = normalized_vec_env.step(action)\n",
        "  print(obs, reward)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.92665876 -0.79704857 -0.99989035]] [-10.]\n",
            "[[-1.29954119 -1.23516889 -1.15320047]] [-2.0166402]\n",
            "[[-1.53443387 -1.44488302 -1.40359283]] [-1.3786463]\n",
            "[[-1.66764047 -1.46782994 -1.42964962]] [-1.2052342]\n",
            "[[-1.76572662 -1.29553902 -1.50305908]] [-1.0920236]\n",
            "[[-1.82834396 -0.62051219 -1.54829839]] [-1.0381112]\n",
            "[[-1.84433408  1.12558007 -1.53561366]] [-0.99533546]\n",
            "[[-1.80433419  2.34630923 -1.46364458]] [-0.93976575]\n",
            "[[-1.7152554   2.57353761 -1.51016821]] [-0.86900634]\n",
            "[[-1.54065759  2.52666704 -1.32601966]] [-0.83797586]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVzKj3nuhgWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "env = NormalizeActionWrapper(env)\n",
        "env = Monitor(env, filename=None, allow_early_resets=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "normalized_vec_env = VecNormalize(env)\n",
        "\n",
        "model = A2C(\"MlpPolicy\", normalized_vec_env, ent_coef=0.0, gamma=0.95, verbose=1)\n",
        "model.learn(int(1e4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhWB_bHpSkas",
        "colab_type": "text"
      },
      "source": [
        "## Wrapper Bonus: changing the observation space: a wrapper for episode of fixed length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBlS9YxYSpJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "class TimeFeatureWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Add remaining time to observation space for fixed length episodes.\n",
        "    See https://arxiv.org/abs/1712.00378 and https://github.com/aravindr93/mjrl/issues/13.\n",
        "\n",
        "    :param env: (gym.Env)\n",
        "    :param max_steps: (int) Max number of steps of an episode\n",
        "        if it is not wrapped in a TimeLimit object.\n",
        "    :param test_mode: (bool) In test mode, the time feature is constant,\n",
        "        equal to zero. This allow to check that the agent did not overfit this feature,\n",
        "        learning a deterministic pre-defined sequence of actions.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, max_steps=1000, test_mode=False):\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "        # Add a time feature to the observation\n",
        "        low, high = env.observation_space.low, env.observation_space.high\n",
        "        low, high= np.concatenate((low, [0])), np.concatenate((high, [1.]))\n",
        "        env.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
        "\n",
        "        super(TimeFeatureWrapper, self).__init__(env)\n",
        "\n",
        "        if isinstance(env, TimeLimit):\n",
        "            self._max_steps = env._max_episode_steps\n",
        "        else:\n",
        "            self._max_steps = max_steps\n",
        "        self._current_step = 0\n",
        "        self._test_mode = test_mode\n",
        "\n",
        "    def reset(self):\n",
        "        self._current_step = 0\n",
        "        return self._get_obs(self.env.reset())\n",
        "\n",
        "    def step(self, action):\n",
        "        self._current_step += 1\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        return self._get_obs(obs), reward, done, info\n",
        "\n",
        "    def _get_obs(self, obs):\n",
        "        \"\"\"\n",
        "        Concatenate the time feature to the current observation.\n",
        "\n",
        "        :param obs: (np.ndarray)\n",
        "        :return: (np.ndarray)\n",
        "        \"\"\"\n",
        "        # Remaining time is more general\n",
        "        time_feature = 1 - (self._current_step / self._max_steps)\n",
        "        if self._test_mode:\n",
        "            time_feature = 1.0\n",
        "        # Optionnaly: concatenate [time_feature, time_feature ** 2]\n",
        "        return np.concatenate((obs, [time_feature]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
