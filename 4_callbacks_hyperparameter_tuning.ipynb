{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4_callbacks_hyperparameter_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/4_callbacks_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i8lIXBiHRYb6"
      },
      "source": [
        "# Stable Baselines Tutorial - Callbacks and hyperparameter tuning\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n",
        "Stable-Baselines: https://github.com/hill-a/stable-baselines\n",
        "\n",
        "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines zoo: https://github.com/araffin/rl-baselines-zoo\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to use *Callbacks* which allow to do monitoring, auto saving, model manipulation, progress bars, ...\n",
        "\n",
        "\n",
        "You will also see that finding good hyperparameters is key to success in RL.\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "owKXXp8rRZI7",
        "colab": {}
      },
      "source": [
        "!apt install swig\n",
        "!pip install tqdm==4.36.1\n",
        "!pip install stable-baselines[mpi]==2.8.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "18ivrnsaSWUn",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from stable_baselines import A2C, SAC, PPO2, TD3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PytOtL9GdmrE"
      },
      "source": [
        "# The importance of hyperparameter tuning\n",
        "\n",
        "When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc. \n",
        "Poor choice of hyper-parameters can lead to poor/unstable convergence. This challenge is compounded by the variability in performance across random seeds (used to initialize the network weights and the environment).\n",
        "\n",
        "Here we demonstrate on a toy example the [Soft Actor Critic](https://arxiv.org/abs/1801.01290) algorithm applied in the Pendulum environment. Note the change in performance between the default and \"tuned\" parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w5oVvYHwdnYv",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate(model, env, num_episodes=100):\n",
        "    # This function will only work for a single Environment\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            action, _states = model.predict(obs)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    return mean_episode_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-a0v3fgwe54j",
        "colab": {}
      },
      "source": [
        "eval_env = gym.make('Pendulum-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5WRR7kmIeqEB",
        "colab": {}
      },
      "source": [
        "default_model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1).learn(8000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jQbDcbEheqWj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e30e241b-6cb7-404f-be55-fa61635b2e92"
      },
      "source": [
        "evaluate(default_model, eval_env, num_episodes=100)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1224.8244208398855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "smMdkZnvfL1g",
        "colab": {}
      },
      "source": [
        "tuned_model = SAC('MlpPolicy', 'Pendulum-v0', batch_size=256, verbose=1, policy_kwargs=dict(layers=[256, 256])).learn(8000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DN05_Io8fMAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d452241e-19ba-4e29-d51f-433d0a4c4677"
      },
      "source": [
        "evaluate(tuned_model, eval_env, num_episodes=100)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-335.3693346865786"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pi9IwxBYVMl8"
      },
      "source": [
        "Exploring hyperparameter tuning is out of the scope (and time schedule) of this tutorial. However, you need to know that we provide tuned hyperparameter in the [rl zoo](https://github.com/araffin/rl-baselines-zoo) as well as automatic hyperparameter optimization using [Optuna](https://github.com/pfnet/optuna).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mQRZlnhFRUns"
      },
      "source": [
        "## Helper functions\n",
        "This is to help the callbacks store variables (as they are function), but this could be also done by passing a class method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdPSiMFzRUnt",
        "colab": {}
      },
      "source": [
        "def get_callback_vars(model, **kwargs): \n",
        "    \"\"\"\n",
        "    Helps store variables for the callback functions\n",
        "    :param model: (BaseRLModel)\n",
        "    :param **kwargs: initial values of the callback variables\n",
        "    \"\"\"\n",
        "    # save the called attribute in the model\n",
        "    if not hasattr(model, \"_callback_vars\"): \n",
        "        model._callback_vars = dict(**kwargs)\n",
        "    else: # check all the kwargs are in the callback variables\n",
        "        for (name, val) in kwargs.items():\n",
        "            if name not in model._callback_vars:\n",
        "                model._callback_vars[name] = val\n",
        "    return model._callback_vars # return dict reference (mutable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "irHk8FXdRUnw"
      },
      "source": [
        "# Callbacks\n",
        "\n",
        "## A functional approach\n",
        "A callback function takes the `locals()` variables and the `globals()` variables from the model, then returns a boolean value for whether or not the training should continue.\n",
        "\n",
        "Thanks to the access to the models variables, in particular `_locals[\"self\"]`, we are able to even change the parameters of the model without halting the training, or changing the model's code.\n",
        "\n",
        "Here we have a simple callback that can only be called twice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5gTXaNLARUnw",
        "colab": {}
      },
      "source": [
        "def simple_callback(_locals, _globals):\n",
        "    \"\"\"\n",
        "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
        "    :param _locals: (dict)\n",
        "    :param _globals: (dict)\n",
        "    \"\"\" \n",
        "    # get callback variables, with default values if unintialized\n",
        "    callback_vars = get_callback_vars(_locals[\"self\"], called=False) \n",
        "    \n",
        "    if not callback_vars[\"called\"]:\n",
        "        print(\"callback - first call\")\n",
        "        callback_vars[\"called\"] = True\n",
        "        return True # returns True, training continues.\n",
        "    else:\n",
        "        print(\"callback - second call\")\n",
        "        return False # returns False, training stops.\n",
        "\n",
        "model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
        "model.learn(8000, callback=simple_callback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "adsKMvDkRUn0"
      },
      "source": [
        "## First example: Auto saving best model\n",
        "In RL, it is quite useful to keep a clean version of a model as you are training, as we can end up with burn-in of a bad policy. This is a typical use case for callback, as they can call the save function of the model, and observe the training over time.\n",
        "\n",
        "Using the monitoring wrapper, we can save statistics of the environment, and use them to determine the mean training reward.\n",
        "This allows us to save the best model while training.\n",
        "\n",
        "Note that this is not the proper way of evaluating an RL agent, you should create an test environment and evaluate the agent performance in the callback. For simplicity, we will be using the training reward as a proxy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1TuYLBEaRUn0",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "\n",
        "def auto_save_callback(_locals, _globals):\n",
        "    \"\"\"\n",
        "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
        "    :param _locals: (dict)\n",
        "    :param _globals: (dict)\n",
        "    \"\"\"\n",
        "    # get callback variables, with default values if unintialized\n",
        "    callback_vars = get_callback_vars(_locals[\"self\"], n_steps=0, best_mean_reward=-np.inf) \n",
        "\n",
        "    # skip every 20 steps\n",
        "    if callback_vars[\"n_steps\"] % 20 == 0:\n",
        "        # Evaluate policy training performance\n",
        "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
        "        if len(x) > 0:\n",
        "            mean_reward = np.mean(y[-100:])\n",
        "\n",
        "            # New best model, you could save the agent here\n",
        "            if mean_reward > callback_vars[\"best_mean_reward\"]:\n",
        "                callback_vars[\"best_mean_reward\"] = mean_reward\n",
        "                # Example for saving best model\n",
        "                print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
        "                _locals['self'].save(log_dir + 'best_model')\n",
        "    callback_vars[\"n_steps\"] += 1\n",
        "    return True\n",
        "\n",
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "model = A2C('MlpPolicy', env, verbose=0)\n",
        "model.learn(total_timesteps=10000, callback=auto_save_callback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mx18FkEORUn3"
      },
      "source": [
        "## Second example: Realtime plotting of performance\n",
        "While training, it is sometimes useful to how the training progresses over time, relative to the episodic reward.\n",
        "For this, Stable-Baselines has [Tensorboard support](https://stable-baselines.readthedocs.io/en/master/guide/tensorboard.html), however this can be very combersome, especially in disk space usage. \n",
        "\n",
        "**NOTE: Unfortunately live plotting does not work out of the box on google colab**\n",
        "\n",
        "Here, we can use callback again, to plot the episodic reward in realtime, using the monitoring wrapper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c0Bu1HWKRUn4",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib notebook\n",
        "\n",
        "def plotting_callback(_locals, _globals):\n",
        "    \"\"\"\n",
        "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
        "    :param _locals: (dict)\n",
        "    :param _globals: (dict)\n",
        "    \"\"\"\n",
        "    # get callback variables, with default values if unintialized\n",
        "    callback_vars = get_callback_vars(_locals[\"self\"], plot=None) \n",
        "    \n",
        "    # get the monitor's data\n",
        "    x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
        "    if callback_vars[\"plot\"] is None: # make the plot\n",
        "        plt.ion()\n",
        "        fig = plt.figure(figsize=(6,3))\n",
        "        ax = fig.add_subplot(111)\n",
        "        line, = ax.plot(x, y)\n",
        "        callback_vars[\"plot\"] = (line, ax, fig)\n",
        "        plt.show()\n",
        "    else: # update and rescale the plot\n",
        "        callback_vars[\"plot\"][0].set_data(x, y)\n",
        "        callback_vars[\"plot\"][-2].relim()\n",
        "        callback_vars[\"plot\"][-2].set_xlim([_locals[\"total_timesteps\"] * -0.02, \n",
        "                                            _locals[\"total_timesteps\"] * 1.02])\n",
        "        callback_vars[\"plot\"][-2].autoscale_view(True,True,True)\n",
        "        callback_vars[\"plot\"][-1].canvas.draw()\n",
        "        \n",
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make('MountainCarContinuous-v0')\n",
        "env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "        \n",
        "model = PPO2('MlpPolicy', env, verbose=0)\n",
        "model.learn(20000, callback=plotting_callback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "49RVX7ieRUn7"
      },
      "source": [
        "## Third example: Progress bar\n",
        "Quality of life improvement are always welcome when developping and using RL. Here, we used [tqdm](https://tqdm.github.io/) to show a progress bar of the training, along with number of timesteps per second and the estimated time remaining to the end of the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pXa8f6FsRUn8",
        "colab": {}
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
        "class progressbar_callback(object):\n",
        "    def __init__(self, total_timesteps): # init object with total timesteps\n",
        "        self.pbar = None\n",
        "        self.total_timesteps = total_timesteps\n",
        "        \n",
        "    def __enter__(self): # create the progress bar and callback, return the callback\n",
        "        self.pbar = tqdm(total=self.total_timesteps)\n",
        "        \n",
        "        def callback_progressbar(local_, global_):\n",
        "            self.pbar.n = local_[\"self\"].num_timesteps\n",
        "            self.pbar.update(0)\n",
        "            \n",
        "        return callback_progressbar\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb): # close the callback\n",
        "        self.pbar.n = self.total_timesteps\n",
        "        self.pbar.update(0)\n",
        "        self.pbar.close()\n",
        "        \n",
        "model = TD3('MlpPolicy', 'Pendulum-v0', verbose=0)\n",
        "with progressbar_callback(2000) as callback: # this the garanties that the tqdm progress bar closes correctly\n",
        "    model.learn(2000, callback=callback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lBF4ij46RUoC"
      },
      "source": [
        "## Forth example: Composition\n",
        "Thanks to the functional nature of callbacks, it is possible to do a composition of callbacks, into a single callback. This means we can auto save our best model, show the progess bar and episodic reward of the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5hU3T9tkRUoD",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "def compose_callback(*callback_funcs): # takes a list of functions, and returns the composed function.\n",
        "    def _callback(_locals, _globals):\n",
        "        continue_training = True\n",
        "        for cb_func in callback_funcs:\n",
        "            if cb_func(_locals, _globals) is False: # as a callback can return None for legacy reasons.\n",
        "                continue_training = False\n",
        "        return continue_training\n",
        "    return _callback\n",
        "\n",
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "model = PPO2('MlpPolicy', env, verbose=0)\n",
        "with progressbar_callback(10000) as progress_callback:\n",
        "    model.learn(10000, callback=compose_callback(progress_callback, plotting_callback, auto_save_callback))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRB4-qIxg_c9",
        "colab_type": "text"
      },
      "source": [
        "## Exercise: Code your own callback\n",
        "\n",
        "\n",
        "The previous examples showed the basics of what is a callback and what you do with it.\n",
        "\n",
        "The goal of this exercise is to create a callback that will evaluate the model using a test environment and save it if this is the best known model.\n",
        "\n",
        "To make things easier, we are going to use a class instead of a function with the magic method `__call__`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOn0Sr3OhC2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EvalCallback(object):\n",
        "  \"\"\"that\n",
        "  Callback for evaluating an agent.\n",
        "  \n",
        "  :param eval_env: (gym.Env) The environment used for initialization\n",
        "  :param n_eval_episodes: (int) The number of episodes to test the agent\n",
        "  :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n",
        "  \"\"\"\n",
        "  def __init(self, eval_env, n_eval_episodes=5, eval_freq=20):\n",
        "    super(EvalCallback, self).__init__()\n",
        "    self.eval_env = eval_env\n",
        "    self.n_eval_episodes = n_eval_episodes\n",
        "    self.eval_freq = eval_freq\n",
        "    self.n_calls = 0\n",
        "    self.best_mean_reward = -np.inf\n",
        "  \n",
        "  def __call__(self, locals_, globals_):\n",
        "    \"\"\"\n",
        "    This method will be called by the model. This is the equivalent to the callback function\n",
        "    used the previous examples.\n",
        "    :param locals_: (dict)\n",
        "    :param globals_: (dict)\n",
        "    :return: (bool)\n",
        "    \"\"\"\n",
        "    # Get the self object of the model\n",
        "    self_ = locals['self']\n",
        "    \n",
        "    if self.n_calls % self.eval_freq == 0:\n",
        "      # === YOU CODE HERE ===#\n",
        "      # Evaluate the agent:\n",
        "      # you need to do self.n_eval_episodes loop using self.eval_env\n",
        "      # hint: you can use self_.predict(obs)\n",
        "      \n",
        "      # Save the agent if needed\n",
        "      # and update self.best_mean_reward\n",
        "      \n",
        "      print(\"Best mean reward: {:.2f}\".format(self.best_mean_reward))\n",
        "      \n",
        "\n",
        "      # ====================== #\n",
        "    \n",
        "    self.n_calls += 1\n",
        "    \n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO0I81jAkQ0z",
        "colab_type": "text"
      },
      "source": [
        "### Test your callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMop3TlkTbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Env used for training\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "# Env for evaluating the agent\n",
        "eval_env gym.make(\"CartPole-v1\")\n",
        "\n",
        "# === YOU CODE HERE ===#\n",
        "# Create the callback object\n",
        "callback = None\n",
        "\n",
        "# Create the RL model\n",
        "model = None\n",
        "\n",
        "# ====================== #\n",
        "\n",
        "# Train the RL model\n",
        "model.learn(int(100000), callback=callback)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wS20a_NfMAh",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "\n",
        "In this notebook we have seen:\n",
        "- that good hyperparameters are key to the success of RL, you should not except the default ones to work on every problems\n",
        "- what is a callback and what you can do with it\n",
        "- how to create your own callback\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA4gCDtogIaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}